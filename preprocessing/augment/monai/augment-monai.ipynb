{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2339b516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T10:37:22.555975Z",
     "iopub.status.busy": "2025-11-05T10:37:22.555302Z",
     "iopub.status.idle": "2025-11-05T10:38:38.476662Z",
     "shell.execute_reply": "2025-11-05T10:38:38.475870Z"
    },
    "papermill": {
     "duration": 75.926409,
     "end_time": "2025-11-05T10:38:38.478152",
     "exception": false,
     "start_time": "2025-11-05T10:37:22.551743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q monai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf71957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T10:38:38.520388Z",
     "iopub.status.busy": "2025-11-05T10:38:38.520010Z",
     "iopub.status.idle": "2025-11-05T10:39:40.351479Z",
     "shell.execute_reply": "2025-11-05T10:39:40.350457Z"
    },
    "papermill": {
     "duration": 61.854914,
     "end_time": "2025-11-05T10:39:40.353253",
     "exception": false,
     "start_time": "2025-11-05T10:38:38.498339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "2025-11-05 10:39:00.956624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762339141.161197      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762339141.231123      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "2025-11-05 10:39:18,598 | INFO | Preprocessed 002 saved to ./preproc/002 with stats {'pmin': 487.17303833007816, 'pmax': 13756.887910156245, 'mean': 3683.96484375, 'std': 2340.64062501}\n",
      "2025-11-05 10:39:23,032 | INFO | Preprocessed 002 saved to ./preproc/002 with stats {'pmin': 397.98068359375003, 'pmax': 13387.878632812452, 'mean': 3646.12353515625, 'std': 2330.84423829125}\n",
      "2025-11-05 10:39:27,747 | INFO | Preprocessed 002 saved to ./preproc/002 with stats {'pmin': 269.74991607666016, 'pmax': 8854.161816406262, 'mean': 2446.445556640625, 'std': 1606.3848877053124}\n",
      "2025-11-05 10:39:32,408 | INFO | Preprocessed 002 saved to ./preproc/002 with stats {'pmin': 261.8028727722168, 'pmax': 8771.104682617197, 'mean': 2387.70166015625, 'std': 1597.6444091896874}\n",
      "2025-11-05 10:39:34,567 | INFO | Slices by split — train:426 val:0 test:426\n",
      "2025-11-05 10:39:36,344 | INFO | Train batch shapes A: torch.Size([8, 1, 192, 192]), B: torch.Size([8, 1, 192, 192])\n",
      "2025-11-05 10:39:38,537 | INFO | Slices by split — train:426 val:0 test:426\n",
      "2025-11-05 10:39:39,922 | INFO | Train batch shapes A: torch.Size([8, 1, 192, 192]), B: torch.Size([8, 1, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MONAI 2D contrastive pipeline for ADNI T1 MRI slices with UNet mask reconstruction targets.\n",
    "\n",
    "Features\n",
    "- 3D preprocessing: RAS orientation, 1.0 mm spacing, intensity clipping by percentiles inside brain, z score in mask,\n",
    "  optional skull strip heuristic when masks are missing, persistent logging of mean and std per case.\n",
    "- 2D axial slice extraction with brain area threshold, fixed center crop, and midline column recording.\n",
    "- Contrastive dataset: each slice yields two stochastic views with synchronized spatial transforms for image and mask\n",
    "  and image only intensity jitter and noise. Includes custom hemispheric mirror transform around anatomical midline.\n",
    "- Reproducibility: global seeds and per view seeds. Logging of transform params and counts.\n",
    "- Validation and test pipelines with deterministic transforms.\n",
    "- Visual QC utilities to inspect original vs two views with mask overlay.\n",
    "\n",
    "Requirements\n",
    "- Python 3.9+\n",
    "- torch >= 2.0, monai >= 1.3, nibabel, scipy, numpy, matplotlib\n",
    "\n",
    "This file is structured to be imported as a module or run as a script for quick sanity checks.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import pathlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    MapTransform,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Rand2DElasticd,\n",
    "    RandShiftIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandGaussianNoised,\n",
    ")\n",
    "from monai.transforms.transform import Randomizable\n",
    "from monai.data import list_data_collate\n",
    "from monai.utils import InterpolateMode, set_determinism\n",
    "\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, binary_opening\n",
    "\n",
    "# ----------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    # paths\n",
    "    data_csv: Optional[str] = None  # optional CSV with columns: image_path, mask_path (optional), patient_id\n",
    "    preproc_dir: str = \"./preproc\"\n",
    "    logs_dir: str = \"./logs\"\n",
    "\n",
    "    # preprocessing\n",
    "    target_spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0)\n",
    "    percentile_clip: Tuple[float, float] = (0.5, 99.5)\n",
    "    skull_strip_when_missing: bool = True\n",
    "\n",
    "    # 2D slicing\n",
    "    crop_size: Tuple[int, int] = (192, 192)\n",
    "    min_brain_area_ratio: float = 0.01  # exclude slices with less than 1 percent foreground\n",
    "\n",
    "    # augmentation probabilities and ranges\n",
    "    rot_deg: float = 15.0\n",
    "    rot_prob: float = 0.5\n",
    "    hflip_prob: float = 0.5\n",
    "    vflip_prob: float = 0.2\n",
    "    elastic_prob: float = 0.3\n",
    "    elastic_spacing_range: Tuple[int, int] = (16, 40)\n",
    "    elastic_magnitude_range: Tuple[float, float] = (2.0, 5.0)\n",
    "    jitter_prob: float = 0.5\n",
    "    jitter_brightness: float = 0.10\n",
    "    jitter_contrast_range: Tuple[float, float] = (0.9, 1.1)\n",
    "    gamma_range: Tuple[float, float] = (0.9, 1.1)\n",
    "    noise_prob: float = 0.3\n",
    "    noise_std_range: Tuple[float, float] = (0.01, 0.05)\n",
    "    mirror_prob: float = 0.3\n",
    "\n",
    "    # loader\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "\n",
    "    # reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Logging and seeds\n",
    "# ----------------------------------------\n",
    "\n",
    "def setup_logging(log_dir: str) -> logging.Logger:\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    logger = logging.getLogger(\"monai2d\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # clear previous handlers\n",
    "    logger.handlers.clear()\n",
    "    fh = logging.FileHandler(os.path.join(log_dir, \"run.log\"))\n",
    "    ch = logging.StreamHandler()\n",
    "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "    fh.setFormatter(fmt)\n",
    "    ch.setFormatter(fmt)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_determinism(seed=seed)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Utilities\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def load_nifti(path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata(dtype=np.float32)\n",
    "    return data, img.affine\n",
    "\n",
    "\n",
    "def save_nifti(path: str, data: np.ndarray, affine: np.ndarray) -> None:\n",
    "    nib.save(nib.Nifti1Image(data.astype(np.float32), affine), path)\n",
    "\n",
    "\n",
    "def compute_brain_mask_simple(vol: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simple foreground heuristic mask on 3D volume (expects channel last).\n",
    "    Steps: Gaussian smooth, global percentile threshold, morphological open/close.\n",
    "    \"\"\"\n",
    "    v = gaussian_filter(vol, sigma=1.0)\n",
    "    thr = np.percentile(v[v > np.percentile(v, 5)], 60)\n",
    "    mask = v > thr\n",
    "    mask = binary_opening(mask, iterations=2)\n",
    "    mask = binary_closing(mask, iterations=2)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def clip_and_normalize_in_mask(vol: np.ndarray, mask: np.ndarray, pmin: float, pmax: float) -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    assert vol.shape == mask.shape\n",
    "    vox = vol[mask > 0]\n",
    "    lo = np.percentile(vox, pmin)\n",
    "    hi = np.percentile(vox, pmax)\n",
    "    v = np.clip(vol, lo, hi)\n",
    "    mu = float(v[mask > 0].mean())\n",
    "    sigma = float(v[mask > 0].std() + 1e-8)\n",
    "    v = (v - mu) / sigma\n",
    "    stats = {\"pmin\": float(lo), \"pmax\": float(hi), \"mean\": mu, \"std\": sigma}\n",
    "    return v.astype(np.float32), stats\n",
    "\n",
    "\n",
    "def get_bbox_2d(mask2d: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "    ys, xs = np.where(mask2d > 0)\n",
    "    if len(xs) == 0:\n",
    "        return 0, mask2d.shape[0], 0, mask2d.shape[1]\n",
    "    return int(ys.min()), int(ys.max()) + 1, int(xs.min()), int(xs.max()) + 1\n",
    "\n",
    "\n",
    "def center_crop_with_bbox(img: np.ndarray, mask: np.ndarray, size: Tuple[int, int]) -> Tuple[np.ndarray, np.ndarray, Tuple[int, int]]:\n",
    "    h, w = img.shape\n",
    "    th, tw = size\n",
    "    y0, y1, x0, x1 = get_bbox_2d(mask)\n",
    "    cy = (y0 + y1) // 2\n",
    "    cx = (x0 + x1) // 2\n",
    "    sy = max(0, cy - th // 2)\n",
    "    sx = max(0, cx - tw // 2)\n",
    "    sy = min(sy, h - th)\n",
    "    sx = min(sx, w - tw)\n",
    "    pad_y0 = max(0, -sy)\n",
    "    pad_x0 = max(0, -sx)\n",
    "    sy = max(0, sy)\n",
    "    sx = max(0, sx)\n",
    "    img_crop = img[sy: sy + th, sx: sx + tw]\n",
    "    mask_crop = mask[sy: sy + th, sx: sx + tw]\n",
    "    offset = (sy, sx)\n",
    "    # zero pad if needed\n",
    "    if img_crop.shape != (th, tw):\n",
    "        out_img = np.zeros((th, tw), dtype=img.dtype)\n",
    "        out_msk = np.zeros((th, tw), dtype=mask.dtype)\n",
    "        out_img[: img_crop.shape[0], : img_crop.shape[1]] = img_crop\n",
    "        out_msk[: mask_crop.shape[0], : mask_crop.shape[1]] = mask_crop\n",
    "        img_crop, mask_crop = out_img, out_msk\n",
    "    return img_crop, mask_crop, offset\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3D preprocessing\n",
    "# ----------------------------------------\n",
    "\n",
    "def preprocess_subject(\n",
    "    image_path: str,\n",
    "    output_dir: str,\n",
    "    mask_path: Optional[str] = None,\n",
    "    cfg: PipelineConfig = PipelineConfig(),\n",
    "    logger: Optional[logging.Logger] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pid = pathlib.Path(image_path).stem.split(\"_\")[1] if \"_\" in pathlib.Path(image_path).stem else pathlib.Path(image_path).stem\n",
    "\n",
    "    data_dict: Dict[str, Any] = {\"image\": image_path}\n",
    "    if mask_path is not None and os.path.exists(mask_path):\n",
    "        data_dict[\"mask\"] = mask_path\n",
    "\n",
    "    # Build transform keys and per-key interpolation modes dynamically (so it works when mask is missing)\n",
    "    keys_pre = list(data_dict.keys())\n",
    "    modes_pre = tuple(InterpolateMode.BILINEAR if k == \"image\" else InterpolateMode.NEAREST for k in keys_pre)\n",
    "\n",
    "    t_pre = Compose([\n",
    "        LoadImaged(keys=keys_pre, allow_missing_keys=True),\n",
    "        EnsureChannelFirstd(keys=keys_pre, allow_missing_keys=True),\n",
    "        Orientationd(keys=keys_pre, axcodes=\"RAS\", allow_missing_keys=True),\n",
    "        Spacingd(keys=keys_pre, pixdim=cfg.target_spacing, mode=modes_pre, allow_missing_keys=True),\n",
    "        EnsureTyped(keys=keys_pre, allow_missing_keys=True),\n",
    "    ])\n",
    "\n",
    "    d = t_pre(data_dict)\n",
    "\n",
    "    img_t = d[\"image\"]  # MetaTensor/Tensor with channel-first [1, H, W, D]\n",
    "    # Robust affine extraction across MONAI versions\n",
    "    _aff = None\n",
    "    try:\n",
    "        if hasattr(img_t, \"meta\") and isinstance(getattr(img_t, \"meta\"), dict) and \"affine\" in img_t.meta:\n",
    "            _aff = img_t.meta[\"affine\"]\n",
    "        elif hasattr(img_t, \"affine\") and img_t.affine is not None:\n",
    "            _aff = img_t.affine\n",
    "    except Exception:\n",
    "        _aff = None\n",
    "    if _aff is None and \"image_meta_dict\" in d and isinstance(d[\"image_meta_dict\"], dict) and \"affine\" in d[\"image_meta_dict\"]:\n",
    "        _aff = d[\"image_meta_dict\"][\"affine\"]\n",
    "    if _aff is None:\n",
    "        _aff = np.eye(4, dtype=np.float32)\n",
    "    \n",
    "    aff = _aff\n",
    "    img = img_t[0].cpu().numpy()  # 3D\n",
    "\n",
    "    if \"mask\" in d:\n",
    "        msk = (d[\"mask\"][0].cpu().numpy() > 0.5).astype(np.uint8)\n",
    "    else:\n",
    "        if cfg.skull_strip_when_missing:\n",
    "            msk = compute_brain_mask_simple(img).astype(np.uint8)\n",
    "        else:\n",
    "            # fallback to nonzero\n",
    "            msk = (img != 0).astype(np.uint8)\n",
    "\n",
    "    # clip and z score inside mask\n",
    "    img_n, stats = clip_and_normalize_in_mask(img, msk, cfg.percentile_clip[0], cfg.percentile_clip[1])\n",
    "\n",
    "    # save\n",
    "    case_dir = os.path.join(output_dir, pid)\n",
    "    os.makedirs(case_dir, exist_ok=True)\n",
    "    out_img = os.path.join(case_dir, \"image_preproc.nii.gz\")\n",
    "    out_msk = os.path.join(case_dir, \"mask_preproc.nii.gz\")\n",
    "    save_nifti(out_img, img_n, aff)\n",
    "    save_nifti(out_msk, msk.astype(np.uint8), aff)\n",
    "\n",
    "    meta = {\n",
    "        \"patient_id\": pid,\n",
    "        \"image_path\": out_img,\n",
    "        \"mask_path\": out_msk,\n",
    "        \"affine\": aff.tolist(),\n",
    "        \"stats\": stats,\n",
    "        \"shape\": list(img_n.shape),\n",
    "    }\n",
    "\n",
    "    if logger is not None:\n",
    "        with open(os.path.join(os.path.dirname(out_img), \"stats.json\"), \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        logger.info(f\"Preprocessed {pid} saved to {case_dir} with stats {stats}\")\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Slice indexing\n",
    "# ----------------------------------------\n",
    "\n",
    "def build_slice_index(preproc_cases: List[Dict[str, Any]], cfg: PipelineConfig, index_json: str) -> List[Dict[str, Any]]:\n",
    "    index: List[Dict[str, Any]] = []\n",
    "    for meta in preproc_cases:\n",
    "        img3d, _ = load_nifti(meta[\"image_path\"])\n",
    "        msk3d, _ = load_nifti(meta[\"mask_path\"])\n",
    "        h, w, d = img3d.shape\n",
    "        mid_col = w // 2\n",
    "        for z in range(d):\n",
    "            m2 = msk3d[:, :, z] > 0\n",
    "            area_ratio = float(m2.mean())\n",
    "            if area_ratio < cfg.min_brain_area_ratio:\n",
    "                continue\n",
    "            index.append({\n",
    "                \"patient_id\": meta[\"patient_id\"],\n",
    "                \"image_path\": meta[\"image_path\"],\n",
    "                \"mask_path\": meta[\"mask_path\"],\n",
    "                \"slice_index\": int(z),\n",
    "                \"midline_col\": int(mid_col),\n",
    "                \"affine\": meta[\"affine\"],\n",
    "                \"shape2d\": [int(h), int(w)],\n",
    "            })\n",
    "    os.makedirs(os.path.dirname(index_json), exist_ok=True)\n",
    "    with open(index_json, \"w\") as f:\n",
    "        json.dump(index, f, indent=2)\n",
    "    return index\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Custom transforms\n",
    "# ----------------------------------------\n",
    "\n",
    "class RandGammaIntensityd(MapTransform, Randomizable):\n",
    "    \"\"\"Random gamma correction for images in [C, H, W] assuming intensities are roughly standard normal.\n",
    "    Applies image <- sign(image) * |image|**gamma with gamma in a range around 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys: Sequence[str], gamma_range: Tuple[float, float] = (0.9, 1.1), prob: float = 0.5) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.gamma_range = gamma_range\n",
    "        self.prob = prob\n",
    "        self._do = False\n",
    "        self._gamma = 1.0\n",
    "\n",
    "    def randomize(self) -> None:\n",
    "        self._do = self.R.random() < self.prob\n",
    "        self._gamma = self.R.uniform(self.gamma_range[0], self.gamma_range[1])\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        d = dict(data)\n",
    "        self.randomize()\n",
    "        if not self._do:\n",
    "            return d\n",
    "        for k in self.keys:  # type: ignore\n",
    "            x = d[k]\n",
    "            # keep sign to be stable around 0 after z score\n",
    "            d[k] = torch.sign(x) * torch.pow(torch.abs(x) + 1e-8, self._gamma)\n",
    "        return d\n",
    "\n",
    "\n",
    "class MirrorAroundMidline2Dd(MapTransform, Randomizable):\n",
    "    \"\"\"Reflect image and mask around anatomical midline column stored in data[\"midline_col\"].\n",
    "    This is distinct from a simple flip since it mirrors around mid column rather than image boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys: Sequence[str], prob: float = 0.3) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.prob = prob\n",
    "        self._do = False\n",
    "\n",
    "    def randomize(self) -> None:\n",
    "        self._do = self.R.random() < self.prob\n",
    "\n",
    "    @staticmethod\n",
    "    def _mirror_np(arr: np.ndarray, mid_col: int) -> np.ndarray:\n",
    "        # arr shape [H, W]\n",
    "        h, w = arr.shape\n",
    "        j = np.arange(w)\n",
    "        j_m = np.clip(2 * mid_col - j, 0, w - 1)\n",
    "        return arr[:, j_m]\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        d = dict(data)\n",
    "        self.randomize()\n",
    "        if not self._do:\n",
    "            return d\n",
    "        mid = int(d.get(\"midline_col\", d[\"image\"].shape[-1] // 2))\n",
    "        for k in self.keys:  # type: ignore\n",
    "            x = d[k]\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                x2d = x[0].cpu().numpy()\n",
    "                y2d = self._mirror_np(x2d, mid)\n",
    "                if k == \"mask\":\n",
    "                    # nearest like behavior by discrete copy\n",
    "                    y2d = (y2d > 0.5).astype(np.float32)\n",
    "                d[k] = torch.from_numpy(y2d[None, ...]).to(x.dtype)\n",
    "            else:\n",
    "                raise ValueError(\"Transform expects tensors with channel first\")\n",
    "        return d\n",
    "\n",
    "\n",
    "class RandGaussianNoiseRanged(MapTransform, Randomizable):\n",
    "    \"\"\"Additive Gaussian noise with std sampled from a range per-call.\n",
    "    Applies to image only; keeps mask untouched by choosing appropriate keys at call site.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys: Sequence[str], std_range: Tuple[float, float] = (0.01, 0.05), prob: float = 0.3, mean: float = 0.0) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.std_range = std_range\n",
    "        self.mean = mean\n",
    "        self.prob = prob\n",
    "        self._do = False\n",
    "        self._std = float(np.mean(std_range))\n",
    "\n",
    "    def randomize(self) -> None:\n",
    "        self._do = self.R.random() < self.prob\n",
    "        self._std = self.R.uniform(self.std_range[0], self.std_range[1])\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        d = dict(data)\n",
    "        self.randomize()\n",
    "        if not self._do:\n",
    "            return d\n",
    "        for k in self.keys:  # type: ignore\n",
    "            x = d[k]\n",
    "            noise = torch.randn_like(x) * self._std + self.mean\n",
    "            d[k] = x + noise\n",
    "        return d\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Transform builders\n",
    "# ----------------------------------------\n",
    "\n",
    "def make_spatial_aug(cfg: PipelineConfig) -> Compose:\n",
    "    rad = math.radians(cfg.rot_deg)\n",
    "    # dictionary transforms expect a tuple of modes aligned with keys order [\"image\", \"mask\"], not a dict\n",
    "    mode_imgmask = (InterpolateMode.BILINEAR, InterpolateMode.NEAREST)\n",
    "    return Compose([\n",
    "        RandAffined(\n",
    "            keys=[\"image\", \"mask\"],\n",
    "            prob=cfg.rot_prob,\n",
    "            rotate_range=(-rad, rad),  # 2D in plane\n",
    "            translate_range=None,\n",
    "            scale_range=None,\n",
    "            mode=mode_imgmask,\n",
    "            padding_mode=\"zeros\",\n",
    "        ),\n",
    "        Rand2DElasticd(\n",
    "            keys=[\"image\", \"mask\"],\n",
    "            prob=cfg.elastic_prob,\n",
    "            spacing=cfg.elastic_spacing_range,\n",
    "            magnitude_range=cfg.elastic_magnitude_range,\n",
    "            mode=mode_imgmask,\n",
    "            padding_mode=\"zeros\",\n",
    "        ),\n",
    "        RandFlipd(keys=[\"image\", \"mask\"], spatial_axis=1, prob=cfg.hflip_prob),  # LR (W axis for 2D [H,W])\n",
    "        RandFlipd(keys=[\"image\", \"mask\"], spatial_axis=0, prob=cfg.vflip_prob),  # AP (H axis for 2D [H,W])\n",
    "        MirrorAroundMidline2Dd(keys=[\"image\", \"mask\"], prob=cfg.mirror_prob),\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_intensity_aug(cfg: PipelineConfig) -> Compose:\n",
    "    return Compose([\n",
    "        RandShiftIntensityd(keys=[\"image\"], offsets=cfg.jitter_brightness, prob=cfg.jitter_prob),\n",
    "        RandScaleIntensityd(keys=[\"image\"], factors=cfg.jitter_contrast_range, prob=cfg.jitter_prob),\n",
    "        RandGammaIntensityd(keys=[\"image\"], gamma_range=cfg.gamma_range, prob=cfg.jitter_prob),\n",
    "        RandGaussianNoiseRanged(keys=[\"image\"], std_range=cfg.noise_std_range, prob=cfg.noise_prob, mean=0.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Dataset\n",
    "# ----------------------------------------\n",
    "\n",
    "class ContrastiveSliceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slice_index: List[Dict[str, Any]],\n",
    "        cfg: PipelineConfig,\n",
    "        logger: Optional[logging.Logger] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> None:\n",
    "        self.idx = slice_index\n",
    "        self.cfg = cfg\n",
    "        self.logger = logger\n",
    "        self.det = deterministic\n",
    "        self.vol_cache: Dict[str, np.ndarray] = {}\n",
    "        self.msk_cache: Dict[str, np.ndarray] = {}\n",
    "\n",
    "        # deterministic transforms for base crop\n",
    "        self.base_crop_size = cfg.crop_size\n",
    "\n",
    "        # augmentation pipelines\n",
    "        self.spatial_aug_a = make_spatial_aug(cfg)\n",
    "        self.intensity_aug_a = make_intensity_aug(cfg)\n",
    "        self.spatial_aug_b = make_spatial_aug(cfg)\n",
    "        self.intensity_aug_b = make_intensity_aug(cfg)\n",
    "\n",
    "        if self.det:\n",
    "            # no random in validation or test\n",
    "            self.spatial_aug_a = Compose([])\n",
    "            self.intensity_aug_a = Compose([])\n",
    "            self.spatial_aug_b = Compose([])\n",
    "            self.intensity_aug_b = Compose([])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx)\n",
    "\n",
    "    def _load_volume(self, path: str) -> np.ndarray:\n",
    "        if path not in self.vol_cache:\n",
    "            arr, _ = load_nifti(path)\n",
    "            self.vol_cache[path] = arr\n",
    "        return self.vol_cache[path]\n",
    "\n",
    "    def _load_mask(self, path: str) -> np.ndarray:\n",
    "        if path not in self.msk_cache:\n",
    "            arr, _ = load_nifti(path)\n",
    "            self.msk_cache[path] = (arr > 0.5).astype(np.uint8)\n",
    "        return self.msk_cache[path]\n",
    "\n",
    "    @staticmethod\n",
    "    def _seed_transforms(t: Compose, seed: int) -> None:\n",
    "        # propagate seed to all randomizable transforms\n",
    "        for tr in t.transforms:\n",
    "            if isinstance(tr, Randomizable):\n",
    "                tr.set_random_state(seed=seed)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, Any]:\n",
    "        item = self.idx[i]\n",
    "        img3d = self._load_volume(item[\"image_path\"])  # H W D\n",
    "        msk3d = self._load_mask(item[\"mask_path\"])     # H W D\n",
    "        z = int(item[\"slice_index\"])\n",
    "        img2d = img3d[:, :, z]\n",
    "        msk2d = msk3d[:, :, z]\n",
    "\n",
    "        # center crop on brain bbox with zero pad\n",
    "        img2d, msk2d, offset = center_crop_with_bbox(img2d, msk2d, self.base_crop_size)\n",
    "        # adjust midline column into cropped coordinates\n",
    "        _, sx = offset\n",
    "        midline_cropped = int(np.clip(int(item[\"midline_col\"]) - sx, 0, self.base_crop_size[1] - 1))\n",
    "\n",
    "        # prepare dict\n",
    "        base = {\n",
    "            \"image\": torch.from_numpy(img2d[None, ...].astype(np.float32)),\n",
    "            \"mask\": torch.from_numpy(msk2d[None, ...].astype(np.float32)),\n",
    "            \"midline_col\": int(midline_cropped),\n",
    "        }\n",
    "\n",
    "        # view A\n",
    "        if not self.det:\n",
    "            seed_a = np.random.randint(0, 2**31 - 1)\n",
    "            self._seed_transforms(self.spatial_aug_a, seed_a)\n",
    "            self._seed_transforms(self.intensity_aug_a, seed_a + 1337)\n",
    "        da = self.spatial_aug_a(dict(base))\n",
    "        da = self.intensity_aug_a(da)\n",
    "\n",
    "        # view B\n",
    "        if not self.det:\n",
    "            seed_b = np.random.randint(0, 2**31 - 1)\n",
    "            self._seed_transforms(self.spatial_aug_b, seed_b)\n",
    "            self._seed_transforms(self.intensity_aug_b, seed_b + 7331)\n",
    "        db = self.spatial_aug_b(dict(base))\n",
    "        db = self.intensity_aug_b(db)\n",
    "\n",
    "        # mask safety: keep binary\n",
    "        da[\"mask\"] = (da[\"mask\"] > 0.5).float()\n",
    "        db[\"mask\"] = (db[\"mask\"] > 0.5).float()\n",
    "\n",
    "        # foreground ratio check\n",
    "        fg_ratio = float(da[\"mask\"].mean().item())\n",
    "        if fg_ratio < 1e-4 and self.logger is not None:\n",
    "            self.logger.warning(f\"Very low foreground ratio in sample {i} pid={item['patient_id']} slice={z}\")\n",
    "\n",
    "        return {\n",
    "            \"image_a\": da[\"image\"],\n",
    "            \"mask_a\": da[\"mask\"],\n",
    "            \"image_b\": db[\"image\"],\n",
    "            \"mask_b\": db[\"mask\"],\n",
    "            \"patient_id\": item[\"patient_id\"],\n",
    "            \"slice_index\": z,\n",
    "            \"midline_col\": int(item[\"midline_col\"]),\n",
    "            \"affine\": np.array(item[\"affine\"], dtype=np.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Data utilities\n",
    "# ----------------------------------------\n",
    "\n",
    "def split_by_patient(cases: List[Dict[str, Any]], train: float = 0.7, val: float = 0.15, seed: int = 42) -> Tuple[List[str], List[str], List[str]]:\n",
    "    pids = sorted({c[\"patient_id\"] for c in cases})\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(pids)\n",
    "    n = len(pids)\n",
    "    n_tr = int(train * n)\n",
    "    n_vl = int(val * n)\n",
    "    tr = pids[:n_tr]\n",
    "    vl = pids[n_tr:n_tr + n_vl]\n",
    "    te = pids[n_tr + n_vl:]\n",
    "    return tr, vl, te\n",
    "\n",
    "\n",
    "def filter_index_by_pids(index: List[Dict[str, Any]], pids: List[str]) -> List[Dict[str, Any]]:\n",
    "    pidset = set(pids)\n",
    "    return [it for it in index if it[\"patient_id\"] in pidset]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Validation and test dataset wrappers\n",
    "# ----------------------------------------\n",
    "\n",
    "def build_dataloaders(\n",
    "    index: List[Dict[str, Any]],\n",
    "    cases_meta: List[Dict[str, Any]],\n",
    "    cfg: PipelineConfig,\n",
    "    logger: Optional[logging.Logger] = None,\n",
    ") -> Dict[str, DataLoader]:\n",
    "    tr_pids, vl_pids, te_pids = split_by_patient(cases_meta, train=0.7, val=0.15, seed=cfg.seed)\n",
    "    idx_tr = filter_index_by_pids(index, tr_pids)\n",
    "    idx_vl = filter_index_by_pids(index, vl_pids)\n",
    "    idx_te = filter_index_by_pids(index, te_pids)\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.info(f\"Slices by split — train:{len(idx_tr)} val:{len(idx_vl)} test:{len(idx_te)}\")\n",
    "\n",
    "    ds_tr = ContrastiveSliceDataset(idx_tr, cfg, logger=logger, deterministic=False)\n",
    "    ds_vl = ContrastiveSliceDataset(idx_vl, cfg, logger=logger, deterministic=True)\n",
    "    ds_te = ContrastiveSliceDataset(idx_te, cfg, logger=logger, deterministic=True)\n",
    "\n",
    "    if len(ds_tr) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training dataset is empty. Provide subjects in demo_subjects or from your CSV, \"\n",
    "            \"lower cfg.min_brain_area_ratio if many slices are filtered, and ensure preprocessing ran.\"\n",
    "        )\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory)\n",
    "    dl_vl = DataLoader(ds_vl, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory)\n",
    "    dl_te = DataLoader(ds_te, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory)\n",
    "\n",
    "    return {\"train\": dl_tr, \"val\": dl_vl, \"test\": dl_te}\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Visualization utilities\n",
    "# ----------------------------------------\n",
    "\n",
    "def _overlay_mask(img: np.ndarray, msk: np.ndarray, alpha: float = 0.4) -> np.ndarray:\n",
    "    img_n = (img - img.min()) / (img.ptp() + 1e-8)\n",
    "    rgb = np.stack([img_n, img_n, img_n], axis=-1)\n",
    "    color = np.array([1.0, 0.0, 0.0])[None, None, :]\n",
    "    rgb = (1 - alpha * msk[..., None]) * rgb + alpha * msk[..., None] * color\n",
    "    return np.clip(rgb, 0, 1)\n",
    "\n",
    "\n",
    "def show_qc_grid(dataset: ContrastiveSliceDataset, indices: Sequence[int] = (0, 1, 2)) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n = len(indices)\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=3, figsize=(12, 4 * n))\n",
    "    if n == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    for r, i in enumerate(indices):\n",
    "        sample = dataset[i]\n",
    "        img0 = sample[\"image_a\"][0].numpy()  # using view A pre spatial, but already augmented; for original, you can bypass aug in code if needed\n",
    "        # For a true original view, you can modify dataset to return base before aug. For brevity, we display A and B plus A with overlay.\n",
    "        img_a = sample[\"image_a\"][0].numpy()\n",
    "        msk_a = sample[\"mask_a\"][0].numpy()\n",
    "        img_b = sample[\"image_b\"][0].numpy()\n",
    "        msk_b = sample[\"mask_b\"][0].numpy()\n",
    "\n",
    "        axes[r, 0].imshow(_overlay_mask(img_a, msk_a > 0.5))\n",
    "        axes[r, 0].set_title(f\"View A pid={sample['patient_id']} z={sample['slice_index']}\")\n",
    "        axes[r, 0].axis(\"off\")\n",
    "        axes[r, 1].imshow(_overlay_mask(img_b, msk_b > 0.5))\n",
    "        axes[r, 1].set_title(\"View B\")\n",
    "        axes[r, 1].axis(\"off\")\n",
    "        axes[r, 2].imshow(img_a, cmap=\"gray\")\n",
    "        axes[r, 2].set_title(\"View A gray\")\n",
    "        axes[r, 2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Script entry: example usage\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def _make_dummy_case(root: str, pid: str = \"DUMMY01\", shape: Tuple[int, int, int] = (192, 192, 64)) -> Dict[str, Any]:\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    h, w, d = shape\n",
    "    # synthetic brain-like blob\n",
    "    yy, xx, zz = np.meshgrid(np.linspace(-1, 1, h), np.linspace(-1, 1, w), np.linspace(-1, 1, d), indexing=\"ij\")\n",
    "    r = np.sqrt(yy**2 + (xx*0.9)**2 + (zz*1.4)**2)\n",
    "    img = np.exp(-3*r**2).astype(np.float32)\n",
    "    img = (img * 2000 + 300)  # shift/scale to mimic T1 range\n",
    "    mask = (r < 0.9).astype(np.uint8)\n",
    "    aff = np.diag([1.0, 1.0, 1.0, 1.0]).astype(np.float32)\n",
    "    case_dir = os.path.join(root, pid)\n",
    "    os.makedirs(case_dir, exist_ok=True)\n",
    "    img_path = os.path.join(case_dir, f\"{pid}_T1.nii.gz\")\n",
    "    msk_path = os.path.join(case_dir, f\"{pid}_mask.nii.gz\")\n",
    "    save_nifti(img_path, img, aff)\n",
    "    save_nifti(msk_path, mask, aff)\n",
    "    return {\"image_path\": img_path, \"mask_path\": msk_path, \"patient_id\": pid}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = PipelineConfig()\n",
    "    logger = setup_logging(cfg.logs_dir)\n",
    "    set_global_seed(cfg.seed)\n",
    "\n",
    "    # demo subject list example (replace with your actual paths)\n",
    "    demo_subjects = [\n",
    "        {\n",
    "            \"image_path\": \"/kaggle/input/1-mri-samples/002_S_0295/002_S_0295/MT1__GradWarp__N3m/2010-05-13_06_37_21.0/I291867/ADNI_002_S_0295_MR_MT1__GradWarp__N3m_Br_20120322162736575_S84944_I291867.nii\", \n",
    "            \"mask_path\": None, \n",
    "            \"patient_id\": \"001\"\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"/kaggle/input/1-mri-samples/002_S_0295/002_S_0295/MT1__GradWarp__N3m/2010-05-13_06_45_21.0/I291869/ADNI_002_S_0295_MR_MT1__GradWarp__N3m_Br_20120322162842799_S84948_I291869.nii\", \n",
    "            \"mask_path\": None, \n",
    "            \"patient_id\": \"001\"\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"/kaggle/input/1-mri-samples/002_S_0413/002_S_0413/MT1__GradWarp__N3m/2010-05-06_12_37_46.0/I291872/ADNI_002_S_0413_MR_MT1__GradWarp__N3m_Br_20120322163151051_S84763_I291872.nii\", \n",
    "            \"mask_path\": None, \n",
    "            \"patient_id\": \"002\"\n",
    "        },\n",
    "        {\n",
    "            \"image_path\": \"/kaggle/input/1-mri-samples/002_S_0413/002_S_0413/MT1__GradWarp__N3m/2010-05-06_12_46_10.0/I291873/ADNI_002_S_0413_MR_MT1__GradWarp__N3m_Br_20120322163254826_S84764_I291873.nii\", \n",
    "            \"mask_path\": None, \n",
    "            \"patient_id\": \"002\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "    if not demo_subjects:\n",
    "        logger.info(\"No demo subjects provided — creating a synthetic dummy case for a smoke test.\")\n",
    "        demo_subjects = [_make_dummy_case(\"./synthetic_cases\", pid=\"SYN001\")]  # quick sanity dataset\n",
    "\n",
    "    preproc_metas: List[Dict[str, Any]] = []\n",
    "    for s in demo_subjects:\n",
    "        meta = preprocess_subject(\n",
    "            image_path=s[\"image_path\"],\n",
    "            mask_path=s.get(\"mask_path\"),\n",
    "            output_dir=cfg.preproc_dir,\n",
    "            cfg=cfg,\n",
    "            logger=logger,\n",
    "        )\n",
    "        if \"patient_id\" in s:\n",
    "            meta[\"patient_id\"] = s[\"patient_id\"]\n",
    "        preproc_metas.append(meta)\n",
    "\n",
    "    index_path = os.path.join(cfg.logs_dir, \"slice_index.json\")\n",
    "    slice_index = build_slice_index(preproc_metas, cfg, index_path)\n",
    "\n",
    "    loaders = build_dataloaders(slice_index, preproc_metas, cfg, logger=logger)\n",
    "\n",
    "    for batch in loaders[\"train\"]:\n",
    "        logger.info(f\"Train batch shapes A: {batch['image_a'].shape}, B: {batch['image_b'].shape}\")\n",
    "        break\n",
    "\n",
    "    # Optional QC display\n",
    "    # ds_train = loaders[\"train\"].dataset  # type: ignore\n",
    "    # show_qc_grid(ds_train, indices=[0, 1, 2])\n",
    "\n",
    "    index_path = os.path.join(cfg.logs_dir, \"slice_index.json\")\n",
    "    slice_index = build_slice_index(preproc_metas, cfg, index_path)\n",
    "\n",
    "    loaders = build_dataloaders(slice_index, preproc_metas, cfg, logger=logger)\n",
    "\n",
    "    # quick sanity check iteration\n",
    "    for batch in loaders[\"train\"]:\n",
    "        # batch contains keys: image_a, mask_a, image_b, mask_b, patient_id, slice_index, midline_col, affine\n",
    "        logger.info(f\"Train batch shapes A: {batch['image_a'].shape}, B: {batch['image_b'].shape}\")\n",
    "        break\n",
    "\n",
    "    # Optional QC display\n",
    "    # ds_train = loaders[\"train\"].dataset  # type: ignore\n",
    "    # show_qc_grid(ds_train, indices=[0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbfdf2",
   "metadata": {
    "papermill": {
     "duration": 0.020651,
     "end_time": "2025-11-05T10:39:40.395732",
     "exception": false,
     "start_time": "2025-11-05T10:39:40.375081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8656034,
     "sourceId": 13620343,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 144.969095,
   "end_time": "2025-11-05T10:39:43.902116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-05T10:37:18.933021",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
